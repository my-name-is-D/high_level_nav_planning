import numpy as np
import copy
from ours.pymdp import maths, utils, inference, control
from ours.pymdp.algos import run_mmp

from envs.modules import next_p_given_a #JUST FOR TESTING PURPOSES

#==== pymdp modified methods ====#
def run_partial_ob_vanilla_fpi(A, obs, num_obs, num_states, partial_ob=None, prior=None, num_iter=10, dF=1.0, dF_tol=0.001):
    """
    Update marginal posterior beliefs over hidden states using mean-field variational inference, via
    fixed point iteration. 

    Parameters
    ----------
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``np.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    obs: numpy 1D array or numpy ndarray of dtype object
        The observation (generated by the environment). If single modality, this should be a 1D ``np.ndarray``
        (one-hot vector representation). If multi-modality, this should be ``np.ndarray`` of dtype object whose entries are 1D one-hot vectors.
    num_obs: list of ints
        List of dimensionalities of each observation modality
    num_states: list of ints
        List of dimensionalities of each observation modality
    prior: numpy ndarray of dtype object, default None
        Prior over hidden states. If absent, prior is set to be the log uniform distribution over hidden states (identical to the 
        initialisation of the posterior)
    num_iter: int, default 10
        Number of variational fixed-point iterations to run until convergence.
    dF: float, default 1.0
        Initial free energy gradient (dF/dt) before updating in the course of gradient descent.
    dF_tol: float, default 0.001
        Threshold value of the time derivative of the variational free energy (dF/dt), to be checked at 
        each iteration. If dF <= dF_tol, the iterations are halted pre-emptively and the final 
        marginal posterior belief(s) is(are) returned
  
    Returns
    ----------
    qs: numpy 1D array, numpy ndarray of dtype object, optional
        Marginal posterior beliefs over hidden states at current timepoint
    """
    if partial_ob != None:
        A = np.array(A[partial_ob])

    # get model dimensions
    n_modalities = len(num_obs)#unused, else would be an issue with partial ob
    n_factors = len(num_states)
    # print('num_states', num_states, A.shape, partial_ob)
    """
    =========== Step 1 ===========
        Loop over the observation modalities and use assumption of independence 
        among observation modalitiesto multiply each modality-specific likelihood 
        onto a single joint likelihood over hidden factors [size num_states]
    """
  
    likelihood = maths.get_joint_likelihood(A, obs, num_states)

    likelihood = maths.spm_log_single(likelihood)

    

    """
    =========== Step 2 ===========
        Create a flat posterior (and prior if necessary)
    """

    qs = np.empty(n_factors, dtype=object)
    for factor in range(n_factors):
        qs[factor] = np.ones(num_states[factor]) / num_states[factor]

    """
    If prior is not provided, initialise prior to be identical to posterior 
    (namely, a flat categorical distribution). Take the logarithm of it (required for 
    FPI algorithm below).
    """
    if prior is None:
        prior = utils.obj_array_uniform(num_states)
        
    prior = maths.spm_log_obj_array(prior) # log the prior


    """
    =========== Step 3 ===========
        Initialize initial free energy
    """
    prev_vfe = maths.calc_free_energy(qs, prior, n_factors)

    """
    =========== Step 4 ===========
        If we have a single factor, we can just add prior and likelihood because there is a unique FE minimum that can reached instantaneously,
        otherwise we run fixed point iteration
    """
    
    if n_factors == 1:

        qL = maths.spm_dot(likelihood, qs, [0])
        qs = utils.to_obj_array(maths.softmax(qL + prior[0]))
        # print('likelihood, qL, qS',likelihood, qL, qs)
        
        return qs

    else:
        """
        =========== Step 5 ===========
        Run the FPI scheme
        """

        curr_iter = 0
        while curr_iter < num_iter and dF >= dF_tol:
            # Initialise variational free energy
            vfe = 0

            # arg_list = [likelihood, list(range(n_factors))]
            # arg_list = arg_list + list(chain(*([qs_i,[i]] for i, qs_i in enumerate(qs)))) + [list(range(n_factors))]
            # LL_tensor = np.einsum(*arg_list)

            qs_all = qs[0]
            for factor in range(n_factors-1):
                qs_all = qs_all[...,None]*qs[factor+1]
            LL_tensor = likelihood * qs_all

            for factor, qs_i in enumerate(qs):
                # qL = np.einsum(LL_tensor, list(range(n_factors)), 1.0/qs_i, [factor], [factor])
                qL = np.einsum(LL_tensor, list(range(n_factors)), [factor])/qs_i
                qs[factor] = maths.softmax(qL + prior[factor])

            # List of orders in which marginal posteriors are sequentially multiplied into the joint likelihood:
            # First order loops over factors starting at index = 0, second order goes in reverse
            # factor_orders = [range(n_factors), range((n_factors - 1), -1, -1)]

            # iteratively marginalize out each posterior marginal from the joint log-likelihood
            # except for the one associated with a given factor
            # for factor_order in factor_orders:
            #     for factor in factor_order:
            #         qL = spm_dot(likelihood, qs, [factor])
            #         qs[factor] = softmax(qL + prior[factor])

            # calculate new free energy
            vfe = maths.calc_free_energy(qs, prior, n_factors, likelihood)

            # stopping condition - time derivative of free energy
            dF = np.abs(prev_vfe - vfe)
            prev_vfe = vfe

            curr_iter += 1

        return qs
    
def update_posterior_states(A, obs, prior=None, partial_ob=None, **kwargs):
    """
    Update marginal posterior over hidden states using mean-field fixed point iteration 
    FPI or Fixed point iteration. 

    See the following links for details:
    http://www.cs.cmu.edu/~guestrin/Class/10708/recitations/r9/VI-view.pdf, slides 13- 18, and http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.137.221&rep=rep1&type=pdf, slides 24 - 38.
    
    Parameters
    ----------
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``np.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    obs: 1D ``numpy.ndarray``, ``numpy.ndarray`` of dtype object, int or tuple
        The observation (generated by the environment). If single modality, this can be a 1D ``np.ndarray``
        (one-hot vector representation) or an ``int`` (observation index)
        If multi-modality, this can be ``np.ndarray`` of dtype object whose entries are 1D one-hot vectors,
        or a tuple (of ``int``)
    prior: 1D ``numpy.ndarray`` or ``numpy.ndarray`` of dtype object, default None
        Prior beliefs about hidden states, to be integrated with the marginal likelihood to obtain
        a posterior distribution. If not provided, prior is set to be equal to a flat categorical distribution (at the level of
        the individual inference functions).
    **kwargs: keyword arguments 
        List of keyword/parameter arguments corresponding to parameter values for the fixed-point iteration
        algorithm ``algos.fpi.run_vanilla_fpi.py``

    Returns
    ----------
    qs: 1D ``numpy.ndarray`` or ``numpy.ndarray`` of dtype object
        Marginal posterior beliefs over hidden states at current timepoint
    """

    num_obs, num_states, num_modalities, num_factors = utils.get_model_dimensions(A = A)

    if prior is not None:
        prior = utils.to_obj_array(prior)

    if partial_ob is None:
        obs = utils.process_observation(obs, num_modalities, num_obs)
    else:
        obs = utils.process_observation(obs, 1, [num_obs[partial_ob]])
    qs = run_partial_ob_vanilla_fpi(A, obs, num_obs, num_states, partial_ob, prior, **kwargs)
    return qs

def update_posterior_states_full(
    A,
    B,
    prev_obs,
    policies,
    prev_actions=None,
    prior=None,
    policy_sep_prior = True,
    partial_ob = None,
    **kwargs,
):

    num_obs, num_states, num_modalities, num_factors = utils.get_model_dimensions(A, B)
    if partial_ob != None:
        A = np.array(A[partial_ob])
        num_obs = [num_obs[partial_ob]]
        num_modalities = 1
    
    proc_obs_seq = utils.obj_array(len(prev_obs))
    for t, obs_t in enumerate(prev_obs):
        if len(obs_t) > 1 and partial_ob != None:
            obs_t = obs_t[partial_ob]
        proc_obs_seq[t] = utils.process_observation(obs_t, num_modalities, num_obs)
    prev_obs = proc_obs_seq
    
    lh_seq = inference.get_joint_likelihood_seq(A, prev_obs, num_states)

    if prev_actions is not None:
        prev_actions = np.stack(prev_actions,0)

    qs_seq_pi = utils.obj_array(len(policies))
    F = np.zeros(len(policies)) # variational free energy of policies

    for p_idx, policy in enumerate(policies):
        # get sequence and the free energy for policy
        qs_seq_pi[p_idx], F[p_idx] = run_mmp(
            lh_seq,
            B,
            policy,
            prev_actions=prev_actions,
            prior= prior[p_idx] if policy_sep_prior else prior, 
            **kwargs
        )
    # print('qs_seq_pi', qs_seq_pi[0].shape)
    return qs_seq_pi, F

def update_state_likelihood_dirichlet(
    pB, B, actions, qs, qs_prev, lr=1.0, factors="all"
):
    """
    Update Dirichlet parameters of the transition distribution. 

    Parameters
    -----------
    pB: ``numpy.ndarray`` of dtype object
        Prior Dirichlet parameters over transition model (same shape as ``B``)
    B: ``numpy.ndarray`` of dtype object
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    actions: 1D ``numpy.ndarray``
        A vector with length equal to the number of control factors, where each element contains the index of the action (for that control factor) performed at 
        a given timestep.
    qs: 1D ``numpy.ndarray`` or ``numpy.ndarray`` of dtype object
        Marginal posterior beliefs over hidden states at current timepoint.
    qs_prev: 1D ``numpy.ndarray`` or ``numpy.ndarray`` of dtype object
        Marginal posterior beliefs over hidden states at previous timepoint.
    lr: float, default ``1.0``
        Learning rate, scale of the Dirichlet pseudo-count update.
    factors: ``list``, default "all"
        Indices (ranging from 0 to ``n_factors - 1``) of the hidden state factors to include 
        in learning. Defaults to "all", meaning that factor-specific sub-arrays of ``pB``
        are all updated using the corresponding hidden state distributions and actions.

    Returns
    -----------
    qB: ``numpy.ndarray`` of dtype object
        Posterior Dirichlet parameters over transition model (same shape as ``B``), after having updated it with state beliefs and actions.
    """

    num_factors = len(pB)

    qB = copy.deepcopy(pB)
    
    if factors == "all":
        factors = list(range(num_factors))

    for factor in factors:
        dfdb = maths.spm_cross(qs[factor], qs_prev[factor])
        #print('update_B: a', actions[factor],'qs[factor]',qs[factor].round(3), 'qs_prev[factor]',qs_prev[factor].round(3))
        # print('dfdb',dfdb)
        dfdb *= (B[factor][:, :, int(actions[factor])] > 0).astype("float")
        qB[factor][:,:,int(actions[factor])] += (lr*dfdb)

    return qB

def update_posterior_policies(
    qs,
    A,
    B,
    C,
    policies,
    use_utility=True,
    use_states_info_gain=True,
    use_param_info_gain=False,
    pA=None,
    pB=None,
    E = None,
    gamma=16.0,
    diff_policy_len = False
):
    """
    Update posterior beliefs about policies by computing expected free energy of each policy and integrating that
    with the prior over policies ``E``. This is intended to be used in conjunction
    with the ``update_posterior_states`` method of the ``inference`` module, since only the posterior about the hidden states at the current timestep
    ``qs`` is assumed to be provided, unconditional on policies. The predictive posterior over hidden states under all policies Q(s, pi) is computed 
    using the starting posterior about states at the current timestep ``qs`` and the generative model (e.g. ``A``, ``B``, ``C``)

    Parameters
    ----------
    qs: ``numpy.ndarray`` of dtype object
        Marginal posterior beliefs over hidden states at current timepoint (unconditioned on policies)
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``numpy.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    B: ``numpy.ndarray`` of dtype object
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    C: ``numpy.ndarray`` of dtype object
       Prior over observations or 'prior preferences', storing the "value" of each outcome in terms of relative log probabilities. 
       This is softmaxed to form a proper probability distribution before being used to compute the expected utility term of the expected free energy.
    policies: ``list`` of 2D ``numpy.ndarray``
        ``list`` that stores each policy in ``policies[p_idx]``. Shape of ``policies[p_idx]`` is ``(num_timesteps, num_factors)`` where `num_timesteps` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.
    use_utility: ``Bool``, default ``True``
        Boolean flag that determines whether expected utility should be incorporated into computation of EFE.
    use_states_info_gain: ``Bool``, default ``True``
        Boolean flag that determines whether state epistemic value (info gain about hidden states) should be incorporated into computation of EFE.
    use_param_info_gain: ``Bool``, default ``False`` 
        Boolean flag that determines whether parameter epistemic value (info gain about generative model parameters) should be incorporated into computation of EFE.
    pA: ``numpy.ndarray`` of dtype object, optional
        Dirichlet parameters over observation model (same shape as ``A``)
    pB: ``numpy.ndarray`` of dtype object, optional
        Dirichlet parameters over transition model (same shape as ``B``)
    E: 1D ``numpy.ndarray``, optional
        Vector of prior probabilities of each policy (what's referred to in the active inference literature as "habits")
    gamma: float, default 16.0
        Prior precision over policies, scales the contribution of the expected free energy to the posterior over policies

    Returns
    ----------
    q_pi: 1D ``numpy.ndarray``
        Posterior beliefs over policies, i.e. a vector containing one posterior probability per policy.
    G: 1D ``numpy.ndarray``
        Negative expected free energies of each policy, i.e. a vector containing one negative expected free energy per policy.
    """

    n_policies = len(policies)
    G = np.zeros(n_policies)
    q_pi = np.zeros((n_policies, 1))

    if E is None:
        lnE = maths.spm_log_single(np.ones(n_policies) / n_policies)
    else:
        lnE =  maths.spm_log_single(E) 

    for idx, policy in enumerate(policies):
        qs_pi =  control.get_expected_states(qs, B, policy)
        qo_pi =  control.get_expected_obs(qs_pi, A)
        policy_length = len(policy)

        if use_utility:
            utility_term = control.calc_expected_utility(qo_pi, C)
            if diff_policy_len : 
                utility_term = utility_term/ policy_length
            G[idx] +=  utility_term

        if use_states_info_gain:
            info_gain =  control.calc_states_info_gain(A, qs_pi)

            if diff_policy_len : 
                info_gain = info_gain/ policy_length
            G[idx] += info_gain

        if use_param_info_gain:
            if pA is not None:
                param_info_gain = control.calc_pA_info_gain(pA, qo_pi, qs_pi)
                if diff_policy_len : 
                    param_info_gain = info_gain/ policy_length
                G[idx] +=  param_info_gain
            if pB is not None:
                param_info_gain = control.calc_pB_info_gain(pB, qs_pi, qs, policy)
                if diff_policy_len : 
                    param_info_gain = info_gain/ policy_length
                G[idx] +=  param_info_gain
    q_pi =  maths.softmax(G * gamma + lnE)    

    return q_pi, G

def update_posterior_policies_test(
    qs,
    A,
    B,
    C,
    policies,
    use_utility=True,
    use_states_info_gain=True,
    use_param_info_gain=False,
    pA=None,
    pB=None,
    E = None,
    gamma=16.0,
    diff_policy_len = False,
):
    """
    Update posterior beliefs about policies by computing expected free energy of each policy and integrating that
    with the prior over policies ``E``. This is intended to be used in conjunction
    with the ``update_posterior_states`` method of the ``inference`` module, since only the posterior about the hidden states at the current timestep
    ``qs`` is assumed to be provided, unconditional on policies. The predictive posterior over hidden states under all policies Q(s, pi) is computed 
    using the starting posterior about states at the current timestep ``qs`` and the generative model (e.g. ``A``, ``B``, ``C``)

    Parameters
    ----------
    qs: ``numpy.ndarray`` of dtype object
        Marginal posterior beliefs over hidden states at current timepoint (unconditioned on policies)
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``numpy.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    B: ``numpy.ndarray`` of dtype object
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    C: ``numpy.ndarray`` of dtype object
       Prior over observations or 'prior preferences', storing the "value" of each outcome in terms of relative log probabilities. 
       This is softmaxed to form a proper probability distribution before being used to compute the expected utility term of the expected free energy.
    policies: ``list`` of 2D ``numpy.ndarray``
        ``list`` that stores each policy in ``policies[p_idx]``. Shape of ``policies[p_idx]`` is ``(num_timesteps, num_factors)`` where `num_timesteps` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.
    use_utility: ``Bool``, default ``True``
        Boolean flag that determines whether expected utility should be incorporated into computation of EFE.
    use_states_info_gain: ``Bool``, default ``True``
        Boolean flag that determines whether state epistemic value (info gain about hidden states) should be incorporated into computation of EFE.
    use_param_info_gain: ``Bool``, default ``False`` 
        Boolean flag that determines whether parameter epistemic value (info gain about generative model parameters) should be incorporated into computation of EFE.
    pA: ``numpy.ndarray`` of dtype object, optional
        Dirichlet parameters over observation model (same shape as ``A``)
    pB: ``numpy.ndarray`` of dtype object, optional
        Dirichlet parameters over transition model (same shape as ``B``)
    E: 1D ``numpy.ndarray``, optional
        Vector of prior probabilities of each policy (what's referred to in the active inference literature as "habits")
    gamma: float, default 16.0
        Prior precision over policies, scales the contribution of the expected free energy to the posterior over policies

    Returns
    ----------
    q_pi: 1D ``numpy.ndarray``
        Posterior beliefs over policies, i.e. a vector containing one posterior probability per policy.
    G: 1D ``numpy.ndarray``
        Negative expected free energies of each policy, i.e. a vector containing one negative expected free energy per policy.
    """

    n_policies = len(policies)
    G = np.zeros(n_policies)
    G_per_actions = np.zeros(n_policies,  dtype=object)
    q_pi = np.zeros((n_policies, 1))

    
    if E is None:
        lnE = maths.spm_log_single(np.ones(n_policies) / n_policies)
    else:
        lnE =  maths.spm_log_single(E) 

    for idx, policy in enumerate(policies):
        qs_pi =  control.get_expected_states(qs, B, policy)
        qo_pi =  control.get_expected_obs(qs_pi, A)
        policy_length = len(policy)
        utility_term_per_action = utils.obj_array_zeros([len(policy)])[0]
        info_gain_per_action = utils.obj_array_zeros([len(policy)])[0]
        
        if use_utility:
            utility_term, utility_term_per_action = calc_expected_utility_test(qo_pi, C)
            # print('policy', from_int_to_str(policy))
            # print('utility_term_per_action', utility_term_per_action)
            if diff_policy_len : 
                utility_term = utility_term/ policy_length
            G[idx] +=  utility_term

        if use_states_info_gain:
            
            info_gain, info_gain_per_action =  calc_states_info_gain_test(A, qs_pi)
                             
          

            if diff_policy_len : 
                info_gain = info_gain/ policy_length
            G[idx] += info_gain

        if use_param_info_gain:
            if pA is not None:
                param_info_gain = control.calc_pA_info_gain(pA, qo_pi, qs_pi)
                if diff_policy_len : 
                    param_info_gain = info_gain/ policy_length
                G[idx] +=  param_info_gain
            if pB is not None:
                param_info_gain = control.calc_pB_info_gain(pB, qs_pi, qs, policy)
                if diff_policy_len : 
                    param_info_gain = info_gain/ policy_length
                G[idx] +=  param_info_gain
        G_per_actions[idx] = utility_term_per_action + info_gain_per_action
    
    q_pi =  maths.softmax(G * gamma + lnE)    
    # G_per_actions = utils.norm_dist_obj_arr(G_per_actions) #normed in 

    return q_pi, G, G_per_actions

def update_posterior_policies_full(
    qs_seq_pi,
    A,
    B,
    C,
    policies,
    use_utility=True,
    use_states_info_gain=True,
    use_param_info_gain=False,
    prior=None,
    pA=None,
    pB=None,
    F = None,
    E = None,
    gamma=16.0,
    diff_policy_len = False
):  
    """
    Update posterior beliefs about policies by computing expected free energy of each policy and integrating that
    with the variational free energy of policies ``F`` and prior over policies ``E``. This is intended to be used in conjunction
    with the ``update_posterior_states_full`` method of ``inference.py``, since the full posterior over future timesteps, under all policies, is
    assumed to be provided in the input array ``qs_seq_pi``.

    Parameters
    ----------
    qs_seq_pi: ``numpy.ndarray`` of dtype object
        Posterior beliefs over hidden states for each policy. Nesting structure is policies, timepoints, factors,
        where e.g. ``qs_seq_pi[p][t][f]`` stores the marginal belief about factor ``f`` at timepoint ``t`` under policy ``p``.
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``numpy.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    B: ``numpy.ndarray`` of dtype object
        Dynamics likelihood mapping or 'transition model', mapping from hidden states at ``t`` to hidden states at ``t+1``, given some control state ``u``.
        Each element ``B[f]`` of this object array stores a 3-D tensor for hidden state factor ``f``, whose entries ``B[f][s, v, u]`` store the probability
        of hidden state level ``s`` at the current time, given hidden state level ``v`` and action ``u`` at the previous time.
    C: ``numpy.ndarray`` of dtype object
       Prior over observations or 'prior preferences', storing the "value" of each outcome in terms of relative log probabilities. 
       This is softmaxed to form a proper probability distribution before being used to compute the expected utility term of the expected free energy.
    policies: ``list`` of 2D ``numpy.ndarray``
        ``list`` that stores each policy in ``policies[p_idx]``. Shape of ``policies[p_idx]`` is ``(num_timesteps, num_factors)`` where `num_timesteps` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.
    use_utility: ``Bool``, default ``True``
        Boolean flag that determines whether expected utility should be incorporated into computation of EFE.
    use_states_info_gain: ``Bool``, default ``True``
        Boolean flag that determines whether state epistemic value (info gain about hidden states) should be incorporated into computation of EFE.
    use_param_info_gain: ``Bool``, default ``False`` 
        Boolean flag that determines whether parameter epistemic value (info gain about generative model parameters) should be incorporated into computation of EFE. 
    prior: ``numpy.ndarray`` of dtype object, default ``None``
        If provided, this is a ``numpy`` object array with one sub-array per hidden state factor, that stores the prior beliefs about initial states. 
        If ``None``, this defaults to a flat (uninformative) prior over hidden states.
    pA: ``numpy.ndarray`` of dtype object, default ``None``
        Dirichlet parameters over observation model (same shape as ``A``)
    pB: ``numpy.ndarray`` of dtype object, default ``None``
        Dirichlet parameters over transition model (same shape as ``B``)
    F: 1D ``numpy.ndarray``, default ``None``
        Vector of variational free energies for each policy
    E: 1D ``numpy.ndarray``, default ``None``
        Vector of prior probabilities of each policy (what's referred to in the active inference literature as "habits"). If ``None``, this defaults to a flat (uninformative) prior over policies.
    gamma: ``float``, default 16.0
        Prior precision over policies, scales the contribution of the expected free energy to the posterior over policies

    Returns
    ----------
    q_pi: 1D ``numpy.ndarray``
        Posterior beliefs over policies, i.e. a vector containing one posterior probability per policy.
    G: 1D ``numpy.ndarray``
        Negative expected free energies of each policy, i.e. a vector containing one negative expected free energy per policy.
    """

    # num_obs, num_states, num_modalities, num_factors = utils.get_model_dimensions(A, B)
    
    num_policies = len(policies)

   

    # horizon = len(qs_seq_pi[0])
    # num_policies = len(qs_seq_pi)
    # qo_seq = utils.obj_array(horizon)
    # for t in range(horizon):
    #     qo_seq[t] = utils.obj_array_zeros(num_obs)

    # initialise expected observations
    # qo_seq_pi = utils.obj_array(num_policies)

    # initialize (negative) expected free energies for all policies
    G = np.zeros(num_policies)

    if F is None:
        F =  maths.spm_log_single(np.ones(num_policies) / num_policies)

    if E is None:
        lnE =  maths.spm_log_single(np.ones(num_policies) / num_policies)
    else:
        lnE = maths.spm_log_single(E) 


    for p_idx, policy in enumerate(policies):
        policy_length = len(policy)

        qs_pi = control.get_expected_states(qs_seq_pi, B, policy)
        qo_pi = control.get_expected_obs(qs_pi, A)
        # str_p = from_int_to_str(policy)
        
        if use_utility:
            utility_term = control.calc_expected_utility(qo_pi, C)
            if diff_policy_len : 
                utility_term = utility_term/ policy_length

            G[p_idx] +=  utility_term
            
        if use_states_info_gain:
            info_gain = control.calc_states_info_gain(A, qs_pi)
            if diff_policy_len : 
                info_gain = info_gain/ policy_length
            G[p_idx] += info_gain
        if use_param_info_gain:
            if pA is not None:
                param_info_gain = control.calc_pA_info_gain(pA, qo_pi, qs_pi)
                if diff_policy_len : 
                    param_info_gain = param_info_gain/ policy_length
                G[p_idx] += param_info_gain

            if pB is not None:
                param_info_gain = control.calc_pB_info_gain(pB, qs_pi, qs_seq_pi, policy)
                if diff_policy_len : 
                    param_info_gain = param_info_gain/ policy_length
                G[p_idx] += param_info_gain

    q_pi = maths.softmax(G * gamma - F + lnE)
    
    return q_pi, G

def update_posterior_policies_full_test(
    qs_seq_pi,
    A,
    B,
    C,
    policies,
    use_utility=True,
    use_states_info_gain=True,
    use_param_info_gain=False,
    prior=None,
    pA=None,
    pB=None,
    F = None,
    E = None,
    gamma=16.0,
    diff_policy_len = False
):  
    """
    Update posterior beliefs about policies by computing expected free energy of each policy and integrating that
    with the variational free energy of policies ``F`` and prior over policies ``E``. This is intended to be used in conjunction
    with the ``update_posterior_states_full`` method of ``inference.py``, since the full posterior over future timesteps, under all policies, is
    assumed to be provided in the input array ``qs_seq_pi``.

    """

    # num_obs, num_states, num_modalities, num_factors = utils.get_model_dimensions(A, B)
    
    num_policies = len(policies)

   

    # horizon = len(qs_seq_pi[0])
    # num_policies = len(qs_seq_pi)
    # qo_seq = utils.obj_array(horizon)
    # for t in range(horizon):
    #     qo_seq[t] = utils.obj_array_zeros(num_obs)

    # initialise expected observations
    # qo_seq_pi = utils.obj_array(num_policies)

    # initialize (negative) expected free energies for all policies
    G = np.zeros(num_policies)
    G_per_actions = np.zeros(num_policies,  dtype=object)
    if F is None:
        F =  maths.spm_log_single(np.ones(num_policies) / num_policies)
    
    if E is None:
        lnE =  maths.spm_log_single(np.ones(num_policies) / num_policies)
    else:
        lnE = maths.spm_log_single(E) 


    for p_idx, policy in enumerate(policies):
        policy_length = len(policy)

        qs_pi = control.get_expected_states(qs_seq_pi, B, policy)
        qo_pi = control.get_expected_obs(qs_pi, A)
        # str_p = from_int_to_str(policy)
        utility_term_per_action = utils.obj_array_zeros([len(policy)])[0]
        info_gain_per_action = utils.obj_array_zeros([len(policy)])[0]

        if use_utility:
            utility_term, utility_term_per_action = calc_expected_utility_test(qo_pi, C)
            if diff_policy_len : 
                utility_term = utility_term/ policy_length

            G[p_idx] +=  utility_term
            
        if use_states_info_gain:
            
            info_gain, info_gain_per_action =  calc_states_info_gain_test(A, qs_pi)
            if diff_policy_len : 
                info_gain = info_gain/ policy_length
            G[p_idx] += info_gain
        if use_param_info_gain:
            if pA is not None:
                param_info_gain = control.calc_pA_info_gain(pA, qo_pi, qs_pi)
                if diff_policy_len : 
                    param_info_gain = param_info_gain/ policy_length
                G[p_idx] += param_info_gain

            if pB is not None:
                param_info_gain = control.calc_pB_info_gain(pB, qs_pi, qs_seq_pi, policy)
                if diff_policy_len : 
                    param_info_gain = param_info_gain/ policy_length
                G[p_idx] += param_info_gain
        G_per_actions[p_idx] = utility_term_per_action + info_gain_per_action

    q_pi = maths.softmax(G * gamma - F + lnE)
    
    return q_pi, G, G_per_actions

def sample_action_test(G_per_actions, q_pi, policies, actions):
    
    """
    Computes the marginal posterior over actions and then samples an action from it, one action per control factor.
    Internal testing version that returns the marginal posterior over actions, and also has a seed argument for reproducibility.

    Parameters
    ----------
    G_per_actions:
    policies: ``list`` of 2D ``numpy.ndarray``
        ``list`` that stores each policy as a 2D array in ``policies[p_idx]``. Shape of ``policies[p_idx]`` 
        is ``(num_timesteps, num_factors)`` where ``num_timesteps`` is the temporal
        depth of the policy and ``num_factors`` is the number of control factors.

    Returns
    ----------
    
    p_actions: ``numpy.ndarray`` of dtype object
        Marginal posteriors over actions, after softmaxing and scaling with action precision. This distribution will be used to sample actions,
        if``action_selection`` argument is "stochastic"
    """
    # for i, arr_i in enumerate(G_per_actions):
    #     print('before before', arr_i)
    #     G_per_actions[i] = -np.exp(-arr_i)
    #     print('after after', G_per_actions[i])
    # G_per_actions = softmax_obj_arr_wt_max(G_per_actions)
    # # G_per_actions = 1/ G_per_actions
    # # for i, arr_i in enumerate(G_per_actions):
    # #     print('before', arr_i)
    # #     G_per_actions[i] = -np.log2(arr_i)
    # #     print('after', G_per_actions[i])
    policy_length = len(policies[0]) #assuming all same lengths
    # print('agent lookahead sample action_test', policy_length)
    action_marginals = np.zeros((policy_length*2+1, policy_length*2+1))

    # # weight each action according to its integrated posterior probability under all policies at the current timestep
    # for pol_idx, policy in enumerate(policies):
    #     pose = [policy_length, policy_length]
    #     for action_idx, action in enumerate(policy):
    #         if int(action) == 4:
    #             continue
    #         pose = next_p_given_a(pose, actions, action)
    #         action_marginals[pose[0]][pose[1]] += G_per_actions[pol_idx][action_idx]
    #         if pose[0] == 4 and pose[1]== 4:
    #             print('action',action, 'G', G_per_actions[pol_idx][action_idx])
    #             print('policy',policy, G_per_actions[pol_idx])
    #             print('action_marginals[pose[0]][pose[1]]', action_marginals[pose[0]][pose[1]])
    # action_marginals = (action_marginals/5) #*10/2
    
    
    #TEST2
    for pol_idx, policy in enumerate(policies):
        pose = [policy_length, policy_length]
        for action_idx, action in enumerate(policy):
            # if int(action) == 4:
            #     continue
            pose = next_p_given_a(pose, actions, action)
            action_marginals[pose[0]][pose[1]] += q_pi[pol_idx]
            # if pose[0] == 4 and pose[1]== 4:
            #     print('action',action, 'G', G_per_actions[pol_idx][action_idx])
            #     print('policy',policy, q_pi[pol_idx], G_per_actions[pol_idx])
            #     print('action_marginals[pose[0]][pose[1]]', action_marginals[pose[0]][pose[1]])
    
    
    action_marginals = action_marginals * 15
    return action_marginals

def softmax_wt_max(dist, max):
    """ 
    Computes the softmax function on a set of values
    """

    output = dist - max
    output = np.exp(output)
    output = output / np.sum(output, axis=0)
    return output

def softmax_obj_arr_wt_max(arr):

    output = utils.obj_array(len(arr))
    max = np.max(list(arr))
    for i, arr_i in enumerate(arr):
        output[i] = softmax_wt_max(arr_i,max)
    
    return output
#==== Update A and B ====#

def set_stationary(mat, idx=-1):
    mat[:,:,idx] = np.eye(mat.shape[0])
    return mat

def create_B_matrix(num_states:int, num_actions:int)->np.ndarray:
    """ 
    generate a Transmition matrix B of shape 
    [n_states, n_states, n_actions] with normalised content
    """
    #create matrix of dim: (st, st-1, action)
    B = np.ones([num_states, num_states, num_actions])
    B /= num_states
    return B

def create_A_matrix(num_ob:list, num_states:list, dim:int=1)->np.ndarray:
    """ 
    generate in one Emission matrix A all the observation/states relationship
    each P(o|s) are of shape 
    [num_ob, n_states] with normalised content
    dim: the number of observations we will feed A with.
    """
    A = utils.obj_array_ones([[num_ob[d]] + [num_states[d]] for d in range(dim)])
    A = A/ num_ob
    return A

def update_B_matrix_size(B:np.ndarray, add:int=1, alter_weights:bool = True)->np.ndarray:
    ''' TODO: Improve this logic... not great. especially for pB
    increase the square matrix B by the value add,
    feed the content of original matrix B in the newly generated one
    all normalised values are re-normalised given new shape.
    alter_weights: If we want the new probabilities to be less probable than already defined states.
    '''
    B_mean_value = 1/B[0].shape[0]
    num_states = B[0].shape[0] + add
    new_B = create_B_matrix(num_states, B[0].shape[-1])
    #new_B -= 0.9/num_states
    
    slices = tuple([slice(dim) for dim in B[0].shape])
    new_B[slices] = B[0]
    if alter_weights:
        new_B[new_B == B_mean_value] = 1/num_states - 0.9/num_states 
    #If there are values that have not been explored yet, 
    # their initial values is diminuished according to number of 
    # the number of states (thus not high proba when never tried)
        
    B[0] = new_B
    return B

def update_A_matrix_size(A, add_ob=0, add_state=0, null_proba = True):
    ''' increase the square matrix A by the values adds,
    add_ob: row add
    add_state: col add
    feed the content of original matrix A in the newly generated one
    '''
    num_ob = A.shape[0] + add_ob
    num_states = A.shape[1] + add_state
    
    new_A = create_A_matrix([num_ob], [num_states], 1)[0]
    if null_proba:
        new_A[:] = 0
    else:
        prev_A_mean_value = 1/A.shape[0]
        new_A[new_A == prev_A_mean_value] = 1/num_ob 
        new_A[new_A == 1/(A.shape[0]*2)] = 1/(num_ob*2)  #We also reduce those proba 
        new_A[:, :A.shape[1]] = 1/(num_ob*2) 
        
    slices = tuple([slice(d) for d in A.shape])
    new_A[slices] = A
    A = new_A
    return A


#==== POLICY GENERATION ====#
def create_policies(lookahead:int, actions:dict, current_pose:list=(0,0), lookahead_distance:bool=False, simple_paths:bool=True)-> list:
    ''' Given current pose, and the goals poses
    we want to explore or reach those goals, 
    generate policies going around in a square perimeter. 
    Parameters
    lookahead(int): how far ahead should we imagine (either steps or policy_length)
    actions(dict): authorised actions
    current_pose(list): where we start from (default: (0,0))
    lookahead_distance(bool): do we consider the lookahead as a dist (True) or num of consecutive steps (False)
    simple_paths(bool): by default the paths have 1 turn max, if we want more complex paths, quadrating full area in number of steps max, set to True
    
    Note simple_paths= False is not compatible with lookahead_distance=True (will be set back to false), to avoid long computation time.
    
    '''
    if lookahead == 13:
        print('here okay')
        policies_lists = define_static_policies(lookahead,actions)
    elif simple_paths:
        goal_poses = define_policies_objectives(current_pose, lookahead)
        policies_lists = []
        #get all the actions leading to the endpoints
        for endpoint in goal_poses:
            action_seq_options = define_policies_to_goal(current_pose, endpoint, actions, lookahead, lookahead_distance)
            policies_lists.extend(action_seq_options)
    else:
        #Is used for a 360degree squared exploration area range
        policies_lists = generate_paths_quadrating_area(lookahead,actions)

    if 'STAY' in actions:
        policies_lists.append(np.array([[actions['STAY']]]*lookahead))

    policies_lists = remove_repetitions(policies_lists)
    return policies_lists

def remove_repetitions(policies):
    unique_policies = {tuple(arr.ravel()) for arr in policies}
    # Convert the set back to a list of arrays
    unique_policies = [np.array(policy).reshape(-1, 1) for policy in unique_policies]
    return unique_policies

def define_policies_objectives(current_pose:list, lookahead:int) ->list:
    """ 
    Full 2D exploration around the agent. 
    All corners of square (dist to agent:lookahead) perimeters around agent set as goal
    """
    goal_poses = []

    goal_poses.append([current_pose[0]+lookahead,  current_pose[1]-lookahead])
    goal_poses.append([current_pose[0]+lookahead,  current_pose[1]+lookahead])
    goal_poses.append([current_pose[0]-lookahead,  current_pose[1]-lookahead])
    goal_poses.append([current_pose[0]-lookahead,  current_pose[1]+lookahead])
    
    return goal_poses

def define_policies_to_goal(start_pose:list, end_pose:list, actions:dict, lookahead:int, lookahead_distance:bool=False)->list:
    '''
    Given the current pose and goal pose establish all the sequence of actions 
    leading TOWARD the objective. 
    This code is valid without considering obstacles. If there are, consider
    expanding the area of possible paths.
    actions(dict): the list of possible actions
    lookahead(int): the distance or number of steps to look forward for
    lookahead_distance(bool): wether the lookahead is to be considered as a distance or number of steps.
    '''
    dx,dy = abs(int(start_pose[0] - end_pose[0])), abs(int(start_pose[1] - end_pose[1])) # destination cell

    #If we want to explore, we want a grid path coverage (squared)
    paths = L_shaped_paths(dx, dy)
        
    action_seq_options = []

    for path in paths:
        path = np.array(path)
        if start_pose[0] > end_pose[0]:
            path[:,0]= -path[:,0]
        if start_pose[1] > end_pose[1]:
            path[:,1]= -path[:,1]

        action_seq = []
        for step in range(1, len(path)):
            x_diff, y_diff = path[step][0] - path[step - 1][0], path[step][1] - path[step - 1][1]

            if x_diff > 0:  # Go forward x
                action_seq.append([actions['DOWN']])
            elif x_diff < 0:  # Go backward x
                action_seq.append([actions['UP']])
            elif y_diff > 0:  # Go forward y
                action_seq.append([actions['RIGHT']])
            elif y_diff < 0:  # Go backward y
                action_seq.append([actions['LEFT']])
            
            #If we want the same number of action in all policies, 
            if lookahead_distance == False and len(action_seq) == lookahead: #not the more optimal, but easiest.
                break

            if 'STAY' in actions:
                # Add a 'STAY' action after each step and append it to action_seq_options
                action_seq_with_stay = action_seq.copy()
                action_seq_with_stay.append([actions['STAY']])
                if len(action_seq_with_stay) < lookahead :
                    action_seq_with_stay.extend([[actions['STAY']]] * (lookahead- len(action_seq_with_stay)))
                action_seq_options.append(np.array(action_seq_with_stay).reshape(len(action_seq_with_stay), 1))
        
        if len(action_seq) < lookahead and 'STAY' in actions:
            action_seq.extend([[actions['STAY']]] *(lookahead- len(action_seq)))
        elif 'STAY' not in actions:
            print('Create policies; We might neeed to implement what to do if the policy < policy_len')
        action_seq_options.append(np.array(action_seq).reshape(len(action_seq), 1))

    return action_seq_options

def L_shaped_paths(dx, dy):
    '''
    Create 1 path going to given dx for each y latitude and vice versa for dy. 
    This limit the path generation to half (opposing sides of the starting agent position) 
    the number of tiles on the outline of the rectangle formed by dx and dy.
    LIMITATION: Works with squarred areas
    '''
    paths = [[[0,0]]]
    paths.append([[0,0]])
    while paths[-1][-1] != [dx,dy] :
        new_paths = []
        for p in range(0, len(paths), 2):
            # Update y-path
            if paths[p + 1][-1][1] < dy:
                new_step_y = paths[p + 1][-1].copy()
                new_step_y[1] += 1
                paths[p + 1].append(new_step_y)
                if p == 0:
                    new_paths.append(paths[p + 1].copy())

            # Update x-path
            if paths[p][-1][0] < dx:
                new_step_x = paths[p][-1].copy()
                new_step_x[0] += 1
                paths[p].append(new_step_x)
                if p == 0:
                    new_paths.append(paths[p].copy())

        paths.extend(new_paths)
        
    return paths

def visited_pose(position, path):
    return position in path

def generate_paths_quadrating_area(lookahead, actions_dict):
    paths = [[[0, 0]]] 
    action_paths = [[]]
    
    # Define the allowed motions 
    # (doesn't matter if doesn't correspond to actions_dict, paths are symmetrically created anyway)
    allowed_actions = {'UP': [1, 0], 'DOWN': [-1, 0], 'RIGHT': [0, 1], 'LEFT': [0, -1]}
    
    def generate_paths_recursively(path, action_path):
        if len(path) == lookahead + 1:
            paths.append(path[:])
            action_paths.append(np.array(action_path).reshape(len(action_path), 1))
            return 
        
        for action, direction in allowed_actions.items():
            action_value = actions_dict[action]
            new_position = [path[-1][0] + direction[0], path[-1][1] + direction[1]]
            
            if not visited_pose(new_position, path):
                new_path = path.copy()
                new_action_path = action_path.copy()
                
                new_path.append(new_position)
                new_action_path.append(action_value) 
               
                if 'STAY' in actions_dict and len(new_path) < lookahead+1: #current pose is in path
                    new_path_wt_stay = new_path.copy()
                    new_action_path_wt_stay = new_action_path.copy()

                    new_path_wt_stay.append(new_path_wt_stay[-1])
                    new_action_path_wt_stay.append(actions_dict['STAY'])
                    if len(new_path_wt_stay) < lookahead +1:
                        new_path_wt_stay.extend([new_path_wt_stay[-1]] * (lookahead+1- len(new_path_wt_stay)))
                        new_action_path_wt_stay.extend([actions_dict['STAY']] * (lookahead- len(new_action_path_wt_stay)))
                    paths.append(new_path_wt_stay[:]) 
                    action_paths.append(np.array(new_action_path_wt_stay).reshape(len(new_action_path_wt_stay), 1))        
                generate_paths_recursively(new_path,new_action_path)
    
    # Start generating paths recursively
    generate_paths_recursively(paths[0], action_paths[0])
    
    return action_paths[1:] #,paths[1:]

def from_int_to_str(policy):
    str_policy = []
    for i, a in enumerate(policy):
        if int(a[0]) == 0:
            a = 'L'
        elif int(a[0]) == 1:
            a = 'R'
        elif int(a[0]) == 2:
            a = 'U'
        elif int(a[0]) == 3:
            a = 'D'
        elif int(a[0]) == 4:
            a = '4'
        str_policy.append(a)
    return str_policy

def define_static_policies(lookahead, actions_dict):
    """ generating 78 policies looping around for a specific test"""
    path1_policy = [[2]]*lookahead
    paths = []
    for step in range(len(path1_policy)):
        if 'STAY' in actions_dict:
            # Add a 'STAY' action after each step and append it to action_seq_options
            action_seq_with_stay = path1_policy[:step].copy()
            action_seq_with_stay.append([actions_dict['STAY']])
            if len(action_seq_with_stay) < lookahead :
                    action_seq_with_stay.extend([[actions_dict['STAY']]] * (lookahead- len(action_seq_with_stay)))
        paths.append(action_seq_with_stay)
    
    path_p_wt_left = path1_policy.copy()
    for i in range(3):
        path_p_wt_left.insert(0,[actions_dict['LEFT']])
        paths.append(path_p_wt_left.copy())
    
    path_p_wt_right = path1_policy.copy()
    for i in range(2):
        path_p_wt_right.insert(0,[actions_dict['RIGHT']])
        paths.append(path_p_wt_right.copy()[:lookahead])

    path2_policy = [[0],[0], [2],[2], [1],[1], [2],[2],[2]]
    path_p_wt_down = path2_policy.copy()
    for i in range(2):
        path_p_wt_down.insert(0,[actions_dict['DOWN']])
        paths.append(path_p_wt_down.copy()[:lookahead])

    looping_back = [[3],[0],[0],[0]]
    for step in range(len(looping_back)):
        path = looping_back[step:] + path2_policy
        # print(path[:13])
        paths.append(path[:lookahead])
        
    for step in range(len(path2_policy)-3):
        paths.append(path2_policy[step:])

    path3_policy = [[1],[1],[1], [2],[2],[2],[2], [0],[0],[0],[2]]

    looping_back = [[0],[0],[3],[3],[1],[1]]

    for step in range(len(looping_back)):
        path = looping_back[step:] + path3_policy
        # print(path[:13])
        paths.append(path[:lookahead])
    
    path_p_wt_down = path3_policy.copy()
    for i in range(2):
        path_p_wt_down.insert(0,[actions_dict['DOWN']])
        paths.append(path_p_wt_down.copy())

    for step in range(len(path3_policy)-1):
        paths.append(path3_policy[step:])

    new_paths = []
    for path in paths:
        new_path = []
        insert_id = -1
        save = []
        # print('path', path)
        for step in path[::-1]:
            # print('step', step, 'inserted', insert_id)
            if step[0] == 2:
                # new_path.insert(insert_id,[3])
                new_path.append([3])
                # print('inserted 3')
            elif step[0] == 3:
                # new_path.insert(insert_id,[2])
                new_path.append([2])
                # print('inserted 2')
            elif step[0] == 0:
                # new_path.insert(insert_id,[1])
                new_path.append([1])
                # print('inserted 1')
            elif step[0] == 1:
                # new_path.insert(insert_id,[0])
                new_path.append([0])
                # print('inserted 0')
            else:
                save = [4]
                #new_path.insert(insert_id,step)
                # insert_id-=1
        if len(save) >0:
            new_path.append(save)
        # print('new path', new_path)
        new_paths.append(new_path)
    paths.extend(new_paths)

    for i,path in enumerate(paths):
        if len(path) < lookahead and 'STAY' in actions_dict:
                path.extend([[actions_dict['STAY']]] *(lookahead- len(path)))
        paths[i] = np.array(path).reshape(len(path), 1)
    
    return paths


def calc_expected_utility_test(qo_pi, C):
    """
    Computes the expected utility of a policy, using the observation distribution expected under that policy and a prior preference vector.

    Parameters
    ----------
    qo_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over observations expected under the policy, where ``qo_pi[t]`` stores the beliefs about
        observations expected under the policy at time ``t``
    C: ``numpy.ndarray`` of dtype object
       Prior over observations or 'prior preferences', storing the "value" of each outcome in terms of relative log probabilities. 
       This is softmaxed to form a proper probability distribution before being used to compute the expected utility.

    Returns
    -------
    expected_util: float
        Utility (reward) expected under the policy in question
    """
    n_steps = len(qo_pi)
    
    # initialise expected utility
    expected_util = 0

    # loop over time points and modalities
    num_modalities = len(C)

    # reformat C to be tiled across timesteps, if it's not already
    modalities_to_tile = [modality_i for modality_i in range(num_modalities) if C[modality_i].ndim == 1]

    # make a deepcopy of C where it has been tiled across timesteps
    C_tiled = copy.deepcopy(C)
    for modality in modalities_to_tile:
        C_tiled[modality] = np.tile(C[modality][:,None], (1, n_steps) )
    
    C_prob = maths.softmax_obj_arr(C_tiled) # convert relative log probabilities into proper probability distribution

    expected_util_actions = utils.obj_array_zeros([n_steps])[0]
 
    for t in range(n_steps):
        for modality in range(num_modalities):

            lnC = maths.spm_log_single(C_prob[modality][:, t])
            eu = qo_pi[t][modality].dot(lnC)
            expected_util += eu
            expected_util_actions[t] += eu

    return expected_util, expected_util_actions


def calc_states_info_gain_test(A, qs_pi):
    """
    Computes the Bayesian surprise or information gain about states of a policy, 
    using the observation model and the hidden state distribution expected under that policy.

    Parameters
    ----------
    A: ``numpy.ndarray`` of dtype object
        Sensory likelihood mapping or 'observation model', mapping from hidden states to observations. Each element ``A[m]`` of
        stores an ``numpy.ndarray`` multidimensional array for observation modality ``m``, whose entries ``A[m][i, j, k, ...]`` store 
        the probability of observation level ``i`` given hidden state levels ``j, k, ...``
    qs_pi: ``list`` of ``numpy.ndarray`` of dtype object
        Predictive posterior beliefs over hidden states expected under the policy, where ``qs_pi[t]`` stores the beliefs about
        hidden states expected under the policy at time ``t``

    Returns
    -------
    states_surprise: float
        Bayesian surprise (about states) or salience expected under the policy in question
    """

    n_steps = len(qs_pi)

    states_surprise = 0
    states_surprise_actions = utils.obj_array_zeros([n_steps])[0]

    for t in range(n_steps):
        ss =  maths.spm_MDP_G(A, qs_pi[t])
        states_surprise += ss
        states_surprise_actions[t] += ss

    return states_surprise, states_surprise_actions